{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2 - Predicting Persuasiveness of Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and loading essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import gensim \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import cluster\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Training and Dataset csv files into Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_org = pd.read_csv(\"P2_Training_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_org = pd.read_csv(\"P2_Testing_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_org.shape, \" \", df_test_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set = df_train_org\n",
    "df_test_set = df_test_org\n",
    "\n",
    "frames = [df_train_set, df_test_set]\n",
    "df = pd.concat(frames, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments that are null are removed from processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['text'].isnull()]\n",
    "df.reset_index(inplace = True, drop = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature - Count\n",
    "* The length of the comment in terms of the words is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    df['count'] = [str(len(i.split())) for i in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "* The comment is cleanized\n",
    "* Digits and numeric forms are removed from the dataset.\n",
    "* The comments are tokenized into words.\n",
    "* The Stopwords from english, punctuations, and other ASCII characters are all removed from the dataset.\n",
    "* Lematization(WordNetLemmatizer) is performed on the dataset to reduced the word to root form if appicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    sent = ''.join(i for i in text if not i.isdigit())\n",
    "    word_sent = word_tokenize(sent)\n",
    "    _stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    st = WordNetLemmatizer()\n",
    "    _stopwords.add(\"'s\")\n",
    "    _stopwords.add(\"'ve\")\n",
    "    _stopwords.add(\"‚Äö√†√ú\")\n",
    "    word_sent = [st.lemmatize(word.lower()) for word in word_sent if word not in _stopwords]\n",
    "    return word_sent\n",
    "\n",
    "for i in df:\n",
    "    df['text_tokenized'] = [preprocessing(i) for i in df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "* The Word2Vec embedding is utilized to identify the embeddings of words in the entire dataset.\n",
    "* The words are represented in numeric embedded forms.\n",
    "* The comments are vectorized and consists of word emneddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(df['text_tokenized'], min_count = 1,size = 100, window = 5) \n",
    "dictionary = gensim.corpora.Dictionary(df['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text_tokenized'].map(lambda d: len(d)) > 0]\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = []\n",
    "for j in df['text_tokenized']:\n",
    "    res = sum(np.array(model[j]))\n",
    "    val = res/len(j)\n",
    "    list_text.append(val)\n",
    "\n",
    "    \n",
    "df['vectorized'] = list_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "* Cosine similarity is the dot product between two vectors.\n",
    "* The cosine similarity is calculated between each CMV/Opinion post and related Reply comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = 0\n",
    "l_cosine = []\n",
    "l_cosine.append(0)\n",
    "for i in range(len(df)-1):\n",
    "    a_author = df['thread_id'][init]\n",
    "    b_author = df['thread_id'][i+1]\n",
    "    if(a_author == b_author):\n",
    "        a_text = df['vectorized'][init]\n",
    "        b_text = df['vectorized'][i+1]\n",
    "        l_cosine.append(cluster.util.cosine_distance(a_text, b_text))\n",
    "    else:\n",
    "        init = i+1\n",
    "        l_cosine.append(0)\n",
    "        \n",
    "df['cosine'] = l_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Score\n",
    "* The feature sentiment score is derived by calculating the sentiment associated with each text field in the dataset.\n",
    "* The polarity of the score is considered as the sentiment of the respective comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_calc(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['sentiment'] = df['text'].apply(sentiment_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedge Frequency\n",
    "* Hedging words are considered soft words, as they make the conversation seem less direct, and to limit or qualify claims.\n",
    "* The feature consists of the frequency of hedge words in certain comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hedges = pd.DataFrame()\n",
    "df_hedges = pd.read_csv(\"hedge_words.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_hedge = []\n",
    "for i in df['text_tokenized']:\n",
    "    val = np.intersect1d(i,df_hedges)\n",
    "    l_hedge.append(len(val))\n",
    "    \n",
    "df['hedge'] = l_hedge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tone Word Count\n",
    "* This lexicon is derived from the \"Tone Word Bank\".\n",
    "###### http://www.foothillfalcons.org/ourpages/auto/2013/8/30/62075757/Tone.pdf\n",
    "* Tone words could be essential to capture diction, viewpoint, subject matter, attitude and personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tone = pd.DataFrame()\n",
    "df_tone = pd.read_csv(\"Tone-words.txt\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_tone = []\n",
    "for i in df['text_tokenized']:\n",
    "    val = np.intersect1d(i,df_tone)\n",
    "    l_tone.append(len(val))\n",
    "    \n",
    "df['tone'] = l_tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Models\n",
    "* Two models are created to train the model and evaluate on the test dataset.\n",
    "1. Random Forest\n",
    "2. Kernel Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "def random_forest_model(x_train,y_train,x_test):\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(x_train,y_train)\n",
    "    # predictions\n",
    "    rfc_predict = rfc.predict(x_test)\n",
    "    return rfc_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel SVM\n",
    "\n",
    "def kernal_SVM(x_train, y_train, x_test):\n",
    "    svclassifier = SVC(kernel='sigmoid')\n",
    "    svclassifier.fit(x_train, y_train)\n",
    "    # prediction\n",
    "    y_pred = svclassifier.predict(x_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_evaluation(y_test,y_pred):\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return roc_auc, fpr, tpr, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(roc_value, fpr, tpr):\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = 'P AUC = %0.2f' % roc_value)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase I - Baseline Features\n",
    "* count - Length of a comment in terms of words.\n",
    "* cosine - Cosine Similarity between each CMV and following RE vectors.\n",
    "* sentiment - Sentiment score associated with the comment.\n",
    "* hedge - frequency of the hedge words in comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['count','cosine','sentiment','hedge']].copy()\n",
    "y = df[['delta']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.13, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_forest_pred = random_forest_model(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,r_forest_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, r_forest_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_svm_pred = kernal_SVM(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,k_svm_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, k_svm_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_1 = pd.DataFrame(r_forest_pred)\n",
    "phase_1.to_csv('Feature_set2_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase II - Baseline Features + Two Additional Features\n",
    "* author_score - native feature of the dataset, it pertains to the score assigned by the author to the comment.\n",
    "* count - Length of the comment in terms of words.\n",
    "* cosine - Dot product between the CMV comment / Original Post and RE comments.\n",
    "* sentiment - Sentiment score associated with respect to the comments.\n",
    "* hedge - Count of the number of hedge words.\n",
    "* tone - Count of number of tone words - derived from \"Tone word bank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['author_score','count','cosine','sentiment','hedge', 'tone']].copy()\n",
    "y = df[['delta']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.13, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_forest_pred = random_forest_model(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,r_forest_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, r_forest_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_svm_pred = kernal_SVM(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,k_svm_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, k_svm_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_2 = pd.DataFrame(r_forest_pred)\n",
    "phase_2.to_csv('Feature_set2_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase III - Custom Features\n",
    "* author_score - native feature of the dataset, it pertains to the score assigned by the author to the comment.\n",
    "* count - Length of the comment in terms of words.\n",
    "* sentiment - Sentiment score associated with respect to the comments.\n",
    "* tone - Count of number of tone words - derived from \"Tone word bank\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['author_score','count','sentiment','tone']].copy()\n",
    "y = df[['delta']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.13, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_forest_pred = random_forest_model(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,r_forest_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, r_forest_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_svm_pred = kernal_SVM(X_train,y_train,X_test)\n",
    "roc_auc, fpr, tpr, accuracy, precision, recall, f1 = metric_evaluation(y_test,k_svm_pred)\n",
    "\n",
    "print('Accuracy: %f' % accuracy)\n",
    "print('Precision: %f' % precision)\n",
    "print('Recall: %f' % recall)\n",
    "print('F1 score: %f' % f1)\n",
    "print('Confusion Matrix: ', confusion_matrix(y_test, k_svm_pred))\n",
    "\n",
    "plot_graph(roc_auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_3 = pd.DataFrame(r_forest_pred)\n",
    "phase_3.to_csv('Feature_set_Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
