{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m9_Vi4HleEm"
   },
   "source": [
    "* Train an RNN on technical papers from https://arvix.org/\n",
    "* Beautiful Soup - parse HTML and XML documents in the form of a document tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92lue10QmRwQ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOkp_a_1ma2X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIvK3Jbtmdum"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "id": "GIGzvPSFmf7o",
    "outputId": "5edb1fd4-0b7f-4109-deba-13bbcaeb44c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "o3NEx4gd3SU7",
    "outputId": "51ab9104-bc64-4c8e-e66e-e16d09ab8d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.3\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uF-JZTHkmwyp"
   },
   "source": [
    "### Query the data and extract information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_Tc_REcyLvG"
   },
   "source": [
    "Download papers with mentioned categories and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5BRfGXZmlQq"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = 'http://export.arxiv.org/api/query'\n",
    "CATEGORIES = [\n",
    "    'Machine Learning',\n",
    "    'Natural Language Processing',\n",
    "    'Neural Networks',\n",
    "    'Artifical Intelligence'\n",
    "]\n",
    "KEYWORDS = [\n",
    "    'neural',\n",
    "    'intelligence'\n",
    "    'network',\n",
    "    'deep'    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEfIF8xenmu5"
   },
   "source": [
    "### Helper function to build the url to extract data\n",
    "- number of pages\n",
    "- offset of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWnMYJQenlbd"
   },
   "outputs": [],
   "source": [
    "def build_url(amount, offset):\n",
    "    categories = ' OR '.join('cat:' + x for x in CATEGORIES)\n",
    "    keywords = ' OR '.join('all:' + x for x in KEYWORDS)\n",
    "\n",
    "    url = BASE_PATH\n",
    "    url += '?search_query=(({}) AND ({}))'.format(categories, keywords)\n",
    "    url += '&max_results={}&offset={}'.format(amount, offset)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQXQiAXNY1Uw"
   },
   "outputs": [],
   "source": [
    "def get_count():\n",
    "    url = build_url(0, 0)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    count = int(soup.find('opensearch:totalresults').string)\n",
    "    print(count, 'papers found')\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "PahQlTNyY3nG",
    "outputId": "d4ec549b-b432-4844-aa8e-bda97135311f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65543 papers found\n"
     ]
    }
   ],
   "source": [
    "num_papers = get_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVDs6pUKY5O_"
   },
   "outputs": [],
   "source": [
    "PAGE_SIZE = 100\n",
    "\n",
    "def fetch_page(amount, offset):\n",
    "    url = build_url(amount, offset)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    for entry in soup.findAll('entry'):\n",
    "        text = entry.find('summary').text\n",
    "        text = text.strip().replace('\\n', ' ')\n",
    "        yield text\n",
    "\n",
    "def fetch_all():\n",
    "    for offset in range(0, num_papers, PAGE_SIZE):\n",
    "        print('Fetch papers {}/{}'.format(offset + PAGE_SIZE, num_papers))\n",
    "        \n",
    "        for page in fetch_page(PAGE_SIZE, offset):\n",
    "            yield page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8nwCEnhMY7jB"
   },
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'arxiv_abstracts.txt'\n",
    "\n",
    "def download_data():\n",
    "    if not os.path.isfile(DOWNLOADED_FILENAME):\n",
    "        with open(DOWNLOADED_FILENAME, 'w') as file_:\n",
    "            for abstract in fetch_all():\n",
    "                file_.write(abstract + '\\n')\n",
    "    with open(DOWNLOADED_FILENAME) as file_:\n",
    "        data = file_.readlines()\n",
    "        \n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ppVcJHfXY92K",
    "outputId": "826b35e8-b69b-4ba7-8f28-f57710017a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch papers 100/65543\n",
      "Fetch papers 200/65543\n",
      "Fetch papers 300/65543\n",
      "Fetch papers 400/65543\n",
      "Fetch papers 500/65543\n",
      "Fetch papers 600/65543\n",
      "Fetch papers 700/65543\n",
      "Fetch papers 800/65543\n",
      "Fetch papers 900/65543\n",
      "Fetch papers 1000/65543\n",
      "Fetch papers 1100/65543\n",
      "Fetch papers 1200/65543\n",
      "Fetch papers 1300/65543\n",
      "Fetch papers 1400/65543\n",
      "Fetch papers 1500/65543\n",
      "Fetch papers 1600/65543\n",
      "Fetch papers 1700/65543\n",
      "Fetch papers 1800/65543\n",
      "Fetch papers 1900/65543\n",
      "Fetch papers 2000/65543\n",
      "Fetch papers 2100/65543\n",
      "Fetch papers 2200/65543\n",
      "Fetch papers 2300/65543\n",
      "Fetch papers 2400/65543\n",
      "Fetch papers 2500/65543\n",
      "Fetch papers 2600/65543\n",
      "Fetch papers 2700/65543\n",
      "Fetch papers 2800/65543\n",
      "Fetch papers 2900/65543\n",
      "Fetch papers 3000/65543\n",
      "Fetch papers 3100/65543\n",
      "Fetch papers 3200/65543\n",
      "Fetch papers 3300/65543\n",
      "Fetch papers 3400/65543\n",
      "Fetch papers 3500/65543\n",
      "Fetch papers 3600/65543\n",
      "Fetch papers 3700/65543\n",
      "Fetch papers 3800/65543\n",
      "Fetch papers 3900/65543\n",
      "Fetch papers 4000/65543\n",
      "Fetch papers 4100/65543\n",
      "Fetch papers 4200/65543\n",
      "Fetch papers 4300/65543\n",
      "Fetch papers 4400/65543\n",
      "Fetch papers 4500/65543\n",
      "Fetch papers 4600/65543\n",
      "Fetch papers 4700/65543\n",
      "Fetch papers 4800/65543\n",
      "Fetch papers 4900/65543\n",
      "Fetch papers 5000/65543\n",
      "Fetch papers 5100/65543\n",
      "Fetch papers 5200/65543\n",
      "Fetch papers 5300/65543\n",
      "Fetch papers 5400/65543\n",
      "Fetch papers 5500/65543\n",
      "Fetch papers 5600/65543\n",
      "Fetch papers 5700/65543\n",
      "Fetch papers 5800/65543\n",
      "Fetch papers 5900/65543\n",
      "Fetch papers 6000/65543\n",
      "Fetch papers 6100/65543\n",
      "Fetch papers 6200/65543\n",
      "Fetch papers 6300/65543\n",
      "Fetch papers 6400/65543\n",
      "Fetch papers 6500/65543\n",
      "Fetch papers 6600/65543\n",
      "Fetch papers 6700/65543\n",
      "Fetch papers 6800/65543\n",
      "Fetch papers 6900/65543\n",
      "Fetch papers 7000/65543\n",
      "Fetch papers 7100/65543\n",
      "Fetch papers 7200/65543\n",
      "Fetch papers 7300/65543\n",
      "Fetch papers 7400/65543\n",
      "Fetch papers 7500/65543\n",
      "Fetch papers 7600/65543\n",
      "Fetch papers 7700/65543\n",
      "Fetch papers 7800/65543\n",
      "Fetch papers 7900/65543\n",
      "Fetch papers 8000/65543\n",
      "Fetch papers 8100/65543\n",
      "Fetch papers 8200/65543\n",
      "Fetch papers 8300/65543\n",
      "Fetch papers 8400/65543\n",
      "Fetch papers 8500/65543\n",
      "Fetch papers 8600/65543\n",
      "Fetch papers 8700/65543\n",
      "Fetch papers 8800/65543\n",
      "Fetch papers 8900/65543\n",
      "Fetch papers 9000/65543\n",
      "Fetch papers 9100/65543\n",
      "Fetch papers 9200/65543\n",
      "Fetch papers 9300/65543\n",
      "Fetch papers 9400/65543\n",
      "Fetch papers 9500/65543\n",
      "Fetch papers 9600/65543\n",
      "Fetch papers 9700/65543\n",
      "Fetch papers 9800/65543\n",
      "Fetch papers 9900/65543\n",
      "Fetch papers 10000/65543\n",
      "Fetch papers 10100/65543\n",
      "Fetch papers 10200/65543\n",
      "Fetch papers 10300/65543\n",
      "Fetch papers 10400/65543\n",
      "Fetch papers 10500/65543\n",
      "Fetch papers 10600/65543\n",
      "Fetch papers 10700/65543\n",
      "Fetch papers 10800/65543\n",
      "Fetch papers 10900/65543\n",
      "Fetch papers 11000/65543\n",
      "Fetch papers 11100/65543\n",
      "Fetch papers 11200/65543\n",
      "Fetch papers 11300/65543\n",
      "Fetch papers 11400/65543\n",
      "Fetch papers 11500/65543\n",
      "Fetch papers 11600/65543\n",
      "Fetch papers 11700/65543\n",
      "Fetch papers 11800/65543\n",
      "Fetch papers 11900/65543\n",
      "Fetch papers 12000/65543\n",
      "Fetch papers 12100/65543\n",
      "Fetch papers 12200/65543\n",
      "Fetch papers 12300/65543\n",
      "Fetch papers 12400/65543\n",
      "Fetch papers 12500/65543\n",
      "Fetch papers 12600/65543\n",
      "Fetch papers 12700/65543\n",
      "Fetch papers 12800/65543\n",
      "Fetch papers 12900/65543\n",
      "Fetch papers 13000/65543\n",
      "Fetch papers 13100/65543\n",
      "Fetch papers 13200/65543\n",
      "Fetch papers 13300/65543\n",
      "Fetch papers 13400/65543\n",
      "Fetch papers 13500/65543\n",
      "Fetch papers 13600/65543\n",
      "Fetch papers 13700/65543\n",
      "Fetch papers 13800/65543\n",
      "Fetch papers 13900/65543\n",
      "Fetch papers 14000/65543\n",
      "Fetch papers 14100/65543\n",
      "Fetch papers 14200/65543\n",
      "Fetch papers 14300/65543\n",
      "Fetch papers 14400/65543\n",
      "Fetch papers 14500/65543\n",
      "Fetch papers 14600/65543\n",
      "Fetch papers 14700/65543\n",
      "Fetch papers 14800/65543\n",
      "Fetch papers 14900/65543\n",
      "Fetch papers 15000/65543\n",
      "Fetch papers 15100/65543\n",
      "Fetch papers 15200/65543\n",
      "Fetch papers 15300/65543\n",
      "Fetch papers 15400/65543\n",
      "Fetch papers 15500/65543\n",
      "Fetch papers 15600/65543\n",
      "Fetch papers 15700/65543\n",
      "Fetch papers 15800/65543\n",
      "Fetch papers 15900/65543\n",
      "Fetch papers 16000/65543\n",
      "Fetch papers 16100/65543\n",
      "Fetch papers 16200/65543\n",
      "Fetch papers 16300/65543\n",
      "Fetch papers 16400/65543\n",
      "Fetch papers 16500/65543\n",
      "Fetch papers 16600/65543\n",
      "Fetch papers 16700/65543\n",
      "Fetch papers 16800/65543\n",
      "Fetch papers 16900/65543\n",
      "Fetch papers 17000/65543\n",
      "Fetch papers 17100/65543\n",
      "Fetch papers 17200/65543\n",
      "Fetch papers 17300/65543\n",
      "Fetch papers 17400/65543\n",
      "Fetch papers 17500/65543\n",
      "Fetch papers 17600/65543\n",
      "Fetch papers 17700/65543\n",
      "Fetch papers 17800/65543\n",
      "Fetch papers 17900/65543\n",
      "Fetch papers 18000/65543\n",
      "Fetch papers 18100/65543\n",
      "Fetch papers 18200/65543\n",
      "Fetch papers 18300/65543\n",
      "Fetch papers 18400/65543\n",
      "Fetch papers 18500/65543\n",
      "Fetch papers 18600/65543\n",
      "Fetch papers 18700/65543\n",
      "Fetch papers 18800/65543\n",
      "Fetch papers 18900/65543\n",
      "Fetch papers 19000/65543\n",
      "Fetch papers 19100/65543\n",
      "Fetch papers 19200/65543\n",
      "Fetch papers 19300/65543\n",
      "Fetch papers 19400/65543\n",
      "Fetch papers 19500/65543\n",
      "Fetch papers 19600/65543\n",
      "Fetch papers 19700/65543\n",
      "Fetch papers 19800/65543\n",
      "Fetch papers 19900/65543\n",
      "Fetch papers 20000/65543\n",
      "Fetch papers 20100/65543\n",
      "Fetch papers 20200/65543\n",
      "Fetch papers 20300/65543\n",
      "Fetch papers 20400/65543\n",
      "Fetch papers 20500/65543\n",
      "Fetch papers 20600/65543\n",
      "Fetch papers 20700/65543\n",
      "Fetch papers 20800/65543\n",
      "Fetch papers 20900/65543\n",
      "Fetch papers 21000/65543\n",
      "Fetch papers 21100/65543\n",
      "Fetch papers 21200/65543\n",
      "Fetch papers 21300/65543\n",
      "Fetch papers 21400/65543\n",
      "Fetch papers 21500/65543\n",
      "Fetch papers 21600/65543\n",
      "Fetch papers 21700/65543\n",
      "Fetch papers 21800/65543\n",
      "Fetch papers 21900/65543\n",
      "Fetch papers 22000/65543\n",
      "Fetch papers 22100/65543\n",
      "Fetch papers 22200/65543\n",
      "Fetch papers 22300/65543\n",
      "Fetch papers 22400/65543\n",
      "Fetch papers 22500/65543\n",
      "Fetch papers 22600/65543\n",
      "Fetch papers 22700/65543\n",
      "Fetch papers 22800/65543\n",
      "Fetch papers 22900/65543\n",
      "Fetch papers 23000/65543\n",
      "Fetch papers 23100/65543\n",
      "Fetch papers 23200/65543\n",
      "Fetch papers 23300/65543\n",
      "Fetch papers 23400/65543\n",
      "Fetch papers 23500/65543\n",
      "Fetch papers 23600/65543\n",
      "Fetch papers 23700/65543\n",
      "Fetch papers 23800/65543\n",
      "Fetch papers 23900/65543\n",
      "Fetch papers 24000/65543\n",
      "Fetch papers 24100/65543\n",
      "Fetch papers 24200/65543\n",
      "Fetch papers 24300/65543\n",
      "Fetch papers 24400/65543\n",
      "Fetch papers 24500/65543\n",
      "Fetch papers 24600/65543\n",
      "Fetch papers 24700/65543\n",
      "Fetch papers 24800/65543\n",
      "Fetch papers 24900/65543\n",
      "Fetch papers 25000/65543\n",
      "Fetch papers 25100/65543\n",
      "Fetch papers 25200/65543\n",
      "Fetch papers 25300/65543\n",
      "Fetch papers 25400/65543\n",
      "Fetch papers 25500/65543\n",
      "Fetch papers 25600/65543\n",
      "Fetch papers 25700/65543\n",
      "Fetch papers 25800/65543\n",
      "Fetch papers 25900/65543\n",
      "Fetch papers 26000/65543\n",
      "Fetch papers 26100/65543\n",
      "Fetch papers 26200/65543\n",
      "Fetch papers 26300/65543\n",
      "Fetch papers 26400/65543\n",
      "Fetch papers 26500/65543\n",
      "Fetch papers 26600/65543\n",
      "Fetch papers 26700/65543\n",
      "Fetch papers 26800/65543\n",
      "Fetch papers 26900/65543\n",
      "Fetch papers 27000/65543\n",
      "Fetch papers 27100/65543\n",
      "Fetch papers 27200/65543\n",
      "Fetch papers 27300/65543\n",
      "Fetch papers 27400/65543\n",
      "Fetch papers 27500/65543\n",
      "Fetch papers 27600/65543\n",
      "Fetch papers 27700/65543\n",
      "Fetch papers 27800/65543\n",
      "Fetch papers 27900/65543\n",
      "Fetch papers 28000/65543\n",
      "Fetch papers 28100/65543\n",
      "Fetch papers 28200/65543\n",
      "Fetch papers 28300/65543\n",
      "Fetch papers 28400/65543\n",
      "Fetch papers 28500/65543\n",
      "Fetch papers 28600/65543\n",
      "Fetch papers 28700/65543\n",
      "Fetch papers 28800/65543\n",
      "Fetch papers 28900/65543\n",
      "Fetch papers 29000/65543\n",
      "Fetch papers 29100/65543\n",
      "Fetch papers 29200/65543\n",
      "Fetch papers 29300/65543\n",
      "Fetch papers 29400/65543\n",
      "Fetch papers 29500/65543\n",
      "Fetch papers 29600/65543\n",
      "Fetch papers 29700/65543\n",
      "Fetch papers 29800/65543\n",
      "Fetch papers 29900/65543\n",
      "Fetch papers 30000/65543\n",
      "Fetch papers 30100/65543\n",
      "Fetch papers 30200/65543\n",
      "Fetch papers 30300/65543\n",
      "Fetch papers 30400/65543\n",
      "Fetch papers 30500/65543\n",
      "Fetch papers 30600/65543\n",
      "Fetch papers 30700/65543\n",
      "Fetch papers 30800/65543\n",
      "Fetch papers 30900/65543\n",
      "Fetch papers 31000/65543\n",
      "Fetch papers 31100/65543\n",
      "Fetch papers 31200/65543\n",
      "Fetch papers 31300/65543\n",
      "Fetch papers 31400/65543\n",
      "Fetch papers 31500/65543\n",
      "Fetch papers 31600/65543\n",
      "Fetch papers 31700/65543\n",
      "Fetch papers 31800/65543\n",
      "Fetch papers 31900/65543\n",
      "Fetch papers 32000/65543\n",
      "Fetch papers 32100/65543\n",
      "Fetch papers 32200/65543\n",
      "Fetch papers 32300/65543\n",
      "Fetch papers 32400/65543\n",
      "Fetch papers 32500/65543\n",
      "Fetch papers 32600/65543\n",
      "Fetch papers 32700/65543\n",
      "Fetch papers 32800/65543\n",
      "Fetch papers 32900/65543\n",
      "Fetch papers 33000/65543\n",
      "Fetch papers 33100/65543\n",
      "Fetch papers 33200/65543\n",
      "Fetch papers 33300/65543\n",
      "Fetch papers 33400/65543\n",
      "Fetch papers 33500/65543\n",
      "Fetch papers 33600/65543\n",
      "Fetch papers 33700/65543\n",
      "Fetch papers 33800/65543\n",
      "Fetch papers 33900/65543\n",
      "Fetch papers 34000/65543\n",
      "Fetch papers 34100/65543\n",
      "Fetch papers 34200/65543\n",
      "Fetch papers 34300/65543\n",
      "Fetch papers 34400/65543\n",
      "Fetch papers 34500/65543\n",
      "Fetch papers 34600/65543\n",
      "Fetch papers 34700/65543\n",
      "Fetch papers 34800/65543\n",
      "Fetch papers 34900/65543\n",
      "Fetch papers 35000/65543\n",
      "Fetch papers 35100/65543\n",
      "Fetch papers 35200/65543\n",
      "Fetch papers 35300/65543\n",
      "Fetch papers 35400/65543\n",
      "Fetch papers 35500/65543\n",
      "Fetch papers 35600/65543\n",
      "Fetch papers 35700/65543\n",
      "Fetch papers 35800/65543\n",
      "Fetch papers 35900/65543\n",
      "Fetch papers 36000/65543\n",
      "Fetch papers 36100/65543\n",
      "Fetch papers 36200/65543\n",
      "Fetch papers 36300/65543\n",
      "Fetch papers 36400/65543\n",
      "Fetch papers 36500/65543\n",
      "Fetch papers 36600/65543\n",
      "Fetch papers 36700/65543\n",
      "Fetch papers 36800/65543\n",
      "Fetch papers 36900/65543\n",
      "Fetch papers 37000/65543\n",
      "Fetch papers 37100/65543\n",
      "Fetch papers 37200/65543\n",
      "Fetch papers 37300/65543\n",
      "Fetch papers 37400/65543\n",
      "Fetch papers 37500/65543\n",
      "Fetch papers 37600/65543\n",
      "Fetch papers 37700/65543\n",
      "Fetch papers 37800/65543\n",
      "Fetch papers 37900/65543\n",
      "Fetch papers 38000/65543\n",
      "Fetch papers 38100/65543\n",
      "Fetch papers 38200/65543\n",
      "Fetch papers 38300/65543\n",
      "Fetch papers 38400/65543\n",
      "Fetch papers 38500/65543\n",
      "Fetch papers 38600/65543\n",
      "Fetch papers 38700/65543\n",
      "Fetch papers 38800/65543\n",
      "Fetch papers 38900/65543\n",
      "Fetch papers 39000/65543\n",
      "Fetch papers 39100/65543\n",
      "Fetch papers 39200/65543\n",
      "Fetch papers 39300/65543\n",
      "Fetch papers 39400/65543\n",
      "Fetch papers 39500/65543\n",
      "Fetch papers 39600/65543\n",
      "Fetch papers 39700/65543\n",
      "Fetch papers 39800/65543\n",
      "Fetch papers 39900/65543\n",
      "Fetch papers 40000/65543\n",
      "Fetch papers 40100/65543\n",
      "Fetch papers 40200/65543\n",
      "Fetch papers 40300/65543\n",
      "Fetch papers 40400/65543\n",
      "Fetch papers 40500/65543\n",
      "Fetch papers 40600/65543\n",
      "Fetch papers 40700/65543\n",
      "Fetch papers 40800/65543\n",
      "Fetch papers 40900/65543\n",
      "Fetch papers 41000/65543\n",
      "Fetch papers 41100/65543\n",
      "Fetch papers 41200/65543\n",
      "Fetch papers 41300/65543\n",
      "Fetch papers 41400/65543\n",
      "Fetch papers 41500/65543\n",
      "Fetch papers 41600/65543\n",
      "Fetch papers 41700/65543\n",
      "Fetch papers 41800/65543\n",
      "Fetch papers 41900/65543\n",
      "Fetch papers 42000/65543\n",
      "Fetch papers 42100/65543\n",
      "Fetch papers 42200/65543\n",
      "Fetch papers 42300/65543\n",
      "Fetch papers 42400/65543\n",
      "Fetch papers 42500/65543\n",
      "Fetch papers 42600/65543\n",
      "Fetch papers 42700/65543\n",
      "Fetch papers 42800/65543\n",
      "Fetch papers 42900/65543\n",
      "Fetch papers 43000/65543\n",
      "Fetch papers 43100/65543\n",
      "Fetch papers 43200/65543\n",
      "Fetch papers 43300/65543\n",
      "Fetch papers 43400/65543\n",
      "Fetch papers 43500/65543\n",
      "Fetch papers 43600/65543\n",
      "Fetch papers 43700/65543\n",
      "Fetch papers 43800/65543\n",
      "Fetch papers 43900/65543\n",
      "Fetch papers 44000/65543\n",
      "Fetch papers 44100/65543\n",
      "Fetch papers 44200/65543\n",
      "Fetch papers 44300/65543\n",
      "Fetch papers 44400/65543\n",
      "Fetch papers 44500/65543\n",
      "Fetch papers 44600/65543\n",
      "Fetch papers 44700/65543\n",
      "Fetch papers 44800/65543\n",
      "Fetch papers 44900/65543\n",
      "Fetch papers 45000/65543\n",
      "Fetch papers 45100/65543\n",
      "Fetch papers 45200/65543\n",
      "Fetch papers 45300/65543\n",
      "Fetch papers 45400/65543\n",
      "Fetch papers 45500/65543\n",
      "Fetch papers 45600/65543\n",
      "Fetch papers 45700/65543\n",
      "Fetch papers 45800/65543\n",
      "Fetch papers 45900/65543\n",
      "Fetch papers 46000/65543\n",
      "Fetch papers 46100/65543\n",
      "Fetch papers 46200/65543\n",
      "Fetch papers 46300/65543\n",
      "Fetch papers 46400/65543\n",
      "Fetch papers 46500/65543\n",
      "Fetch papers 46600/65543\n",
      "Fetch papers 46700/65543\n",
      "Fetch papers 46800/65543\n",
      "Fetch papers 46900/65543\n",
      "Fetch papers 47000/65543\n",
      "Fetch papers 47100/65543\n",
      "Fetch papers 47200/65543\n",
      "Fetch papers 47300/65543\n",
      "Fetch papers 47400/65543\n",
      "Fetch papers 47500/65543\n",
      "Fetch papers 47600/65543\n",
      "Fetch papers 47700/65543\n",
      "Fetch papers 47800/65543\n",
      "Fetch papers 47900/65543\n",
      "Fetch papers 48000/65543\n",
      "Fetch papers 48100/65543\n",
      "Fetch papers 48200/65543\n",
      "Fetch papers 48300/65543\n",
      "Fetch papers 48400/65543\n",
      "Fetch papers 48500/65543\n",
      "Fetch papers 48600/65543\n",
      "Fetch papers 48700/65543\n",
      "Fetch papers 48800/65543\n",
      "Fetch papers 48900/65543\n",
      "Fetch papers 49000/65543\n",
      "Fetch papers 49100/65543\n",
      "Fetch papers 49200/65543\n",
      "Fetch papers 49300/65543\n",
      "Fetch papers 49400/65543\n",
      "Fetch papers 49500/65543\n",
      "Fetch papers 49600/65543\n",
      "Fetch papers 49700/65543\n",
      "Fetch papers 49800/65543\n",
      "Fetch papers 49900/65543\n",
      "Fetch papers 50000/65543\n",
      "Fetch papers 50100/65543\n",
      "Fetch papers 50200/65543\n",
      "Fetch papers 50300/65543\n",
      "Fetch papers 50400/65543\n",
      "Fetch papers 50500/65543\n",
      "Fetch papers 50600/65543\n",
      "Fetch papers 50700/65543\n",
      "Fetch papers 50800/65543\n",
      "Fetch papers 50900/65543\n",
      "Fetch papers 51000/65543\n",
      "Fetch papers 51100/65543\n",
      "Fetch papers 51200/65543\n",
      "Fetch papers 51300/65543\n",
      "Fetch papers 51400/65543\n",
      "Fetch papers 51500/65543\n",
      "Fetch papers 51600/65543\n",
      "Fetch papers 51700/65543\n",
      "Fetch papers 51800/65543\n",
      "Fetch papers 51900/65543\n",
      "Fetch papers 52000/65543\n",
      "Fetch papers 52100/65543\n",
      "Fetch papers 52200/65543\n",
      "Fetch papers 52300/65543\n",
      "Fetch papers 52400/65543\n",
      "Fetch papers 52500/65543\n",
      "Fetch papers 52600/65543\n",
      "Fetch papers 52700/65543\n",
      "Fetch papers 52800/65543\n",
      "Fetch papers 52900/65543\n",
      "Fetch papers 53000/65543\n",
      "Fetch papers 53100/65543\n",
      "Fetch papers 53200/65543\n",
      "Fetch papers 53300/65543\n",
      "Fetch papers 53400/65543\n",
      "Fetch papers 53500/65543\n",
      "Fetch papers 53600/65543\n",
      "Fetch papers 53700/65543\n",
      "Fetch papers 53800/65543\n",
      "Fetch papers 53900/65543\n",
      "Fetch papers 54000/65543\n",
      "Fetch papers 54100/65543\n",
      "Fetch papers 54200/65543\n",
      "Fetch papers 54300/65543\n",
      "Fetch papers 54400/65543\n",
      "Fetch papers 54500/65543\n",
      "Fetch papers 54600/65543\n",
      "Fetch papers 54700/65543\n",
      "Fetch papers 54800/65543\n",
      "Fetch papers 54900/65543\n",
      "Fetch papers 55000/65543\n",
      "Fetch papers 55100/65543\n",
      "Fetch papers 55200/65543\n",
      "Fetch papers 55300/65543\n",
      "Fetch papers 55400/65543\n",
      "Fetch papers 55500/65543\n",
      "Fetch papers 55600/65543\n",
      "Fetch papers 55700/65543\n",
      "Fetch papers 55800/65543\n",
      "Fetch papers 55900/65543\n",
      "Fetch papers 56000/65543\n",
      "Fetch papers 56100/65543\n",
      "Fetch papers 56200/65543\n",
      "Fetch papers 56300/65543\n",
      "Fetch papers 56400/65543\n",
      "Fetch papers 56500/65543\n",
      "Fetch papers 56600/65543\n",
      "Fetch papers 56700/65543\n",
      "Fetch papers 56800/65543\n",
      "Fetch papers 56900/65543\n",
      "Fetch papers 57000/65543\n",
      "Fetch papers 57100/65543\n",
      "Fetch papers 57200/65543\n",
      "Fetch papers 57300/65543\n",
      "Fetch papers 57400/65543\n",
      "Fetch papers 57500/65543\n",
      "Fetch papers 57600/65543\n",
      "Fetch papers 57700/65543\n",
      "Fetch papers 57800/65543\n",
      "Fetch papers 57900/65543\n",
      "Fetch papers 58000/65543\n",
      "Fetch papers 58100/65543\n",
      "Fetch papers 58200/65543\n",
      "Fetch papers 58300/65543\n",
      "Fetch papers 58400/65543\n",
      "Fetch papers 58500/65543\n",
      "Fetch papers 58600/65543\n",
      "Fetch papers 58700/65543\n",
      "Fetch papers 58800/65543\n",
      "Fetch papers 58900/65543\n",
      "Fetch papers 59000/65543\n",
      "Fetch papers 59100/65543\n",
      "Fetch papers 59200/65543\n",
      "Fetch papers 59300/65543\n",
      "Fetch papers 59400/65543\n",
      "Fetch papers 59500/65543\n",
      "Fetch papers 59600/65543\n",
      "Fetch papers 59700/65543\n",
      "Fetch papers 59800/65543\n",
      "Fetch papers 59900/65543\n",
      "Fetch papers 60000/65543\n",
      "Fetch papers 60100/65543\n",
      "Fetch papers 60200/65543\n",
      "Fetch papers 60300/65543\n",
      "Fetch papers 60400/65543\n",
      "Fetch papers 60500/65543\n",
      "Fetch papers 60600/65543\n",
      "Fetch papers 60700/65543\n",
      "Fetch papers 60800/65543\n",
      "Fetch papers 60900/65543\n",
      "Fetch papers 61000/65543\n",
      "Fetch papers 61100/65543\n",
      "Fetch papers 61200/65543\n",
      "Fetch papers 61300/65543\n",
      "Fetch papers 61400/65543\n",
      "Fetch papers 61500/65543\n",
      "Fetch papers 61600/65543\n",
      "Fetch papers 61700/65543\n",
      "Fetch papers 61800/65543\n",
      "Fetch papers 61900/65543\n",
      "Fetch papers 62000/65543\n",
      "Fetch papers 62100/65543\n",
      "Fetch papers 62200/65543\n",
      "Fetch papers 62300/65543\n",
      "Fetch papers 62400/65543\n",
      "Fetch papers 62500/65543\n",
      "Fetch papers 62600/65543\n",
      "Fetch papers 62700/65543\n",
      "Fetch papers 62800/65543\n",
      "Fetch papers 62900/65543\n",
      "Fetch papers 63000/65543\n",
      "Fetch papers 63100/65543\n",
      "Fetch papers 63200/65543\n",
      "Fetch papers 63300/65543\n",
      "Fetch papers 63400/65543\n",
      "Fetch papers 63500/65543\n",
      "Fetch papers 63600/65543\n",
      "Fetch papers 63700/65543\n",
      "Fetch papers 63800/65543\n",
      "Fetch papers 63900/65543\n",
      "Fetch papers 64000/65543\n",
      "Fetch papers 64100/65543\n",
      "Fetch papers 64200/65543\n",
      "Fetch papers 64300/65543\n",
      "Fetch papers 64400/65543\n",
      "Fetch papers 64500/65543\n",
      "Fetch papers 64600/65543\n",
      "Fetch papers 64700/65543\n",
      "Fetch papers 64800/65543\n",
      "Fetch papers 64900/65543\n",
      "Fetch papers 65000/65543\n",
      "Fetch papers 65100/65543\n",
      "Fetch papers 65200/65543\n",
      "Fetch papers 65300/65543\n",
      "Fetch papers 65400/65543\n",
      "Fetch papers 65500/65543\n",
      "Fetch papers 65600/65543\n"
     ]
    }
   ],
   "source": [
    "data = download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "fpMtAKrOZMe5",
    "outputId": "41249645-f8ba-4517-edf9-876ad6d67d96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62433"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4avRNQyyyoI"
   },
   "source": [
    "### Number of time steps used when we train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHQ6x0nT4mRE"
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZT2gf_N4nrz"
   },
   "outputs": [],
   "source": [
    "VOCABULARY = \\\n",
    "        \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \\\n",
    "        \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iF6jTP2e4o3T"
   },
   "outputs": [],
   "source": [
    "lookup = {x: i for i, x in enumerate(VOCABULARY)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "1pn_uDB34qQz",
    "outputId": "382ad21c-be3b-4814-ebba-f9a5372bc185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 67),\n",
       " ('v', 75),\n",
       " ('1', 12),\n",
       " ('g', 60),\n",
       " ('j', 63),\n",
       " ('s', 72),\n",
       " ('p', 69),\n",
       " ('e', 58),\n",
       " ('(', 4),\n",
       " ('E', 29)]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lookup = random.sample(lookup.items(), 10)\n",
    "sample_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cUvmzkA11jQB"
   },
   "source": [
    "### One-hot representation of each character\n",
    "* Every window of characters is of MAX_SEQUENCE_LENGTH (50)\n",
    "* Each character is represented in one-hot notation\n",
    "* Every feature vector has length equal to number of characters in the \n",
    "\n",
    "---\n",
    "\n",
    "vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8SU9lSh4rkS"
   },
   "outputs": [],
   "source": [
    "def one_hot(batch, sequence_length = MAX_SEQUENCE_LENGTH):\n",
    "    one_hot_batch = np.zeros((len(batch), sequence_length, len(VOCABULARY)))\n",
    "\n",
    "    # Iterate through every line of text in a batch\n",
    "    for index, line in enumerate(batch):\n",
    "        line = [x for x in line if x in lookup]\n",
    "#         assert 2 <= len(line) <= MAX_SEQUENCE_LENGTH\n",
    "        \n",
    "        # Iterate through every character in a line\n",
    "        for offset, character in enumerate(line):\n",
    "            # Code is the index of the character in the vocabulary\n",
    "            code = lookup[character]\n",
    " \n",
    "            one_hot_batch[index, offset, code] = 1\n",
    "    \n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxTEXuC018fu"
   },
   "source": [
    "### Sliding window over every line\n",
    "* Generate batches of characters for training data\n",
    "* Start the sliding window at index 0 for every line\n",
    "* Slide the window over till the last character in the line is included\n",
    "* Have a stride of MAX_SEQUENCE_LENGTH // 2 for every window move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfBQWZpo4tED"
   },
   "outputs": [],
   "source": [
    "def next_batch():\n",
    "    windows = []\n",
    "    for line in data:\n",
    "        for i in range(0, len(line) - MAX_SEQUENCE_LENGTH + 1, MAX_SEQUENCE_LENGTH // 2):\n",
    "            windows.append(line[i: i + MAX_SEQUENCE_LENGTH])\n",
    "\n",
    "    while True:\n",
    "        random.shuffle(windows)\n",
    "        for i in range(0, len(windows), BATCH_SIZE):\n",
    "            batch = windows[i: i + BATCH_SIZE]\n",
    "            yield one_hot(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "eQtMVMZp4uNT",
    "outputId": "59543150-1934-46cb-97d5-6a1aa9c97e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 100, 83)\n"
     ]
    }
   ],
   "source": [
    "test_batch = None\n",
    "for batch in next_batch():\n",
    "    test_batch = batch\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doeJkq634vcD"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSfp0ugH41ET"
   },
   "outputs": [],
   "source": [
    "sequence = tf.placeholder(tf.float32, [None, MAX_SEQUENCE_LENGTH, len(VOCABULARY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPyAJIdg43jz"
   },
   "outputs": [],
   "source": [
    "X = tf.slice(sequence, (0, 0, 0), (-1, MAX_SEQUENCE_LENGTH - 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMu1ldpp44vD"
   },
   "outputs": [],
   "source": [
    "y = tf.slice(sequence, (0, 1, 0), (-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "MOJev-F-45-z",
    "outputId": "76f05302-1cdf-4746-d39f-56bc943d6338"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(99), Dimension(83)])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "unaZg1IH47RT",
    "outputId": "9b9b4f40-448e-4207-9fb3-3f1b81b7a2d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(99), Dimension(83)])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDlOne6e48zT"
   },
   "outputs": [],
   "source": [
    "def get_mask(target):\n",
    "    mask = tf.compat.v1.reduce_max(tf.abs(target), reduction_indices=2)\n",
    "    return mask\n",
    "\n",
    "def get_sequence_length(target):\n",
    "    mask = get_mask(target)\n",
    "    sequence_length = tf.compat.v1.reduce_sum(mask, reduction_indices=1)\n",
    "    \n",
    "    return sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyPCnzP4499T"
   },
   "outputs": [],
   "source": [
    "num_neurons = 200\n",
    "cell_layers = 2\n",
    "\n",
    "num_steps = MAX_SEQUENCE_LENGTH - 1\n",
    "num_classes = len(VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qY7KvGYO4_hU"
   },
   "outputs": [],
   "source": [
    "sequence_length = get_sequence_length(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zX8dvyUV5A2T"
   },
   "outputs": [],
   "source": [
    "def build_rnn(data, num_steps, sequence_length, initial=None):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_neurons)\n",
    "\n",
    "    multi_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [tf.compat.v1.nn.rnn_cell.GRUCell(num_neurons) for _ in range(cell_layers)])\n",
    "\n",
    "    output, state = tf.nn.dynamic_rnn(\n",
    "        inputs=data,\n",
    "        cell=multi_cell,\n",
    "        dtype=tf.float32,\n",
    "        initial_state=initial,\n",
    "        sequence_length=sequence_length)\n",
    "\n",
    "    # Shared softmax layer across all RNN cells\n",
    "    weight = tf.Variable(tf.truncated_normal([num_neurons, num_classes], stddev=0.01))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "\n",
    "    flattened_output = tf.reshape(output, [-1, num_neurons])\n",
    "\n",
    "    prediction = tf.nn.softmax(tf.matmul(flattened_output, weight) + bias)\n",
    "    prediction = tf.reshape(prediction, [-1, num_steps, num_classes])\n",
    "\n",
    "    return prediction, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "id": "lLPA76Wi5LPk",
    "outputId": "35b4d23a-f141-4d99-abdb-a6e1b711cf98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-5fded447fdd2>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-30-5fded447fdd2>:5: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-30-5fded447fdd2>:12: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "prediction, _ = build_rnn(X, num_steps, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMFgMuUu5M_U"
   },
   "outputs": [],
   "source": [
    "mask = get_mask(y)\n",
    "\n",
    "prediction = tf.clip_by_value(prediction, 1e-10, 1.0)\n",
    "\n",
    "cross_entropy = y * tf.log(prediction)\n",
    "cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "\n",
    "cross_entropy *= mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V16RU96D5Ox0"
   },
   "outputs": [],
   "source": [
    "length = tf.reduce_sum(sequence_length, 0)\n",
    "\n",
    "cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1) / length\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEFOjW3i5P9j"
   },
   "outputs": [],
   "source": [
    "logprob = tf.multiply(prediction, y)\n",
    "logprob = tf.reduce_max(logprob, reduction_indices=2)\n",
    "logprob = tf.log(tf.clip_by_value(logprob, 1e-10, 1.0)) / tf.log(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQg4c4cw5RMk"
   },
   "outputs": [],
   "source": [
    "logprob *= mask\n",
    "\n",
    "length = tf.reduce_sum(sequence_length, 0)\n",
    "\n",
    "logprob = tf.reduce_sum(logprob, reduction_indices=1) / length\n",
    "logprob = tf.reduce_mean(logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "JOhC1ha55SYU",
    "outputId": "c898c89e-31f2-4c3d-fd46-43353eb3df90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(0.002)\n",
    "\n",
    "gradient = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "optimize = optimizer.apply_gradients(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUA2qSjd5T-0"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "epoch_size = 50\n",
    "\n",
    "logprob_evals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TERY_kQ5V3E"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './sample_checkpoint_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cp0fVRl4ww19",
    "outputId": "cea82306-1e19-409f-a19c-2b4665921fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 perplexity 1.0178\n",
      "Epoch  1 perplexity 1.0178\n",
      "Epoch  2 perplexity 1.0178\n",
      "Epoch  3 perplexity 1.0166\n",
      "Epoch  4 perplexity 1.0126\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch  5 perplexity 1.0124\n",
      "Epoch  6 perplexity 1.0124\n",
      "Epoch  7 perplexity 1.0124\n",
      "Epoch  8 perplexity 1.0123\n",
      "Epoch  9 perplexity 1.0123\n",
      "Epoch 10 perplexity 1.0123\n",
      "Epoch 11 perplexity 1.0123\n",
      "Epoch 12 perplexity 1.0123\n",
      "Epoch 13 perplexity 1.0123\n",
      "Epoch 14 perplexity 1.0123\n",
      "Epoch 15 perplexity 1.0122\n",
      "Epoch 16 perplexity 1.0121\n",
      "Epoch 17 perplexity 1.0119\n",
      "Epoch 18 perplexity 1.0115\n",
      "Epoch 19 perplexity 1.0109\n",
      "Epoch 20 perplexity 1.0106\n",
      "Epoch 21 perplexity 1.0103\n",
      "Epoch 22 perplexity 1.0100\n",
      "Epoch 23 perplexity 1.0097\n",
      "Epoch 24 perplexity 1.0094\n",
      "Epoch 25 perplexity 1.0092\n",
      "Epoch 26 perplexity 1.0089\n",
      "Epoch 27 perplexity 1.0087\n",
      "Epoch 28 perplexity 1.0085\n",
      "Epoch 29 perplexity 1.0083\n",
      "Epoch 30 perplexity 1.0081\n",
      "Epoch 31 perplexity 1.0079\n",
      "Epoch 32 perplexity 1.0077\n",
      "Epoch 33 perplexity 1.0076\n",
      "Epoch 34 perplexity 1.0074\n",
      "Epoch 35 perplexity 1.0073\n",
      "Epoch 36 perplexity 1.0071\n",
      "Epoch 37 perplexity 1.0070\n",
      "Epoch 38 perplexity 1.0068\n",
      "Epoch 39 perplexity 1.0067\n",
      "Epoch 40 perplexity 1.0066\n",
      "Epoch 41 perplexity 1.0065\n",
      "Epoch 42 perplexity 1.0064\n",
      "Epoch 43 perplexity 1.0063\n",
      "Epoch 44 perplexity 1.0062\n",
      "Epoch 45 perplexity 1.0061\n",
      "Epoch 46 perplexity 1.0060\n",
      "Epoch 47 perplexity 1.0059\n",
      "Epoch 48 perplexity 1.0058\n",
      "Epoch 49 perplexity 1.0057\n",
      "Epoch 50 perplexity 1.0057\n",
      "Epoch 51 perplexity 1.0056\n",
      "Epoch 52 perplexity 1.0055\n",
      "Epoch 53 perplexity 1.0054\n",
      "Epoch 54 perplexity 1.0054\n",
      "Epoch 55 perplexity 1.0053\n",
      "Epoch 56 perplexity 1.0052\n",
      "Epoch 57 perplexity 1.0052\n",
      "Epoch 58 perplexity 1.0051\n",
      "Epoch 59 perplexity 1.0051\n",
      "Epoch 60 perplexity 1.0050\n",
      "Epoch 61 perplexity 1.0049\n",
      "Epoch 62 perplexity 1.0049\n",
      "Epoch 63 perplexity 1.0048\n",
      "Epoch 64 perplexity 1.0047\n",
      "Epoch 65 perplexity 1.0047\n",
      "Epoch 66 perplexity 1.0047\n",
      "Epoch 67 perplexity 1.0046\n",
      "Epoch 68 perplexity 1.0046\n",
      "Epoch 69 perplexity 1.0045\n",
      "Epoch 70 perplexity 1.0045\n",
      "Epoch 71 perplexity 1.0044\n",
      "Epoch 72 perplexity 1.0044\n",
      "Epoch 73 perplexity 1.0043\n",
      "Epoch 74 perplexity 1.0043\n",
      "Epoch 75 perplexity 1.0042\n",
      "Epoch 76 perplexity 1.0042\n",
      "Epoch 77 perplexity 1.0042\n",
      "Epoch 78 perplexity 1.0041\n",
      "Epoch 79 perplexity 1.0041\n",
      "Epoch 80 perplexity 1.0040\n",
      "Epoch 81 perplexity 1.0040\n",
      "Epoch 82 perplexity 1.0039\n",
      "Epoch 83 perplexity 1.0039\n",
      "Epoch 84 perplexity 1.0038\n",
      "Epoch 85 perplexity 1.0038\n",
      "Epoch 86 perplexity 1.0038\n",
      "Epoch 87 perplexity 1.0037\n",
      "Epoch 88 perplexity 1.0037\n",
      "Epoch 89 perplexity 1.0036\n",
      "Epoch 90 perplexity 1.0036\n",
      "Epoch 91 perplexity 1.0035\n",
      "Epoch 92 perplexity 1.0035\n",
      "Epoch 93 perplexity 1.0034\n",
      "Epoch 94 perplexity 1.0034\n",
      "Epoch 95 perplexity 1.0034\n",
      "Epoch 96 perplexity 1.0033\n",
      "Epoch 97 perplexity 1.0033\n",
      "Epoch 98 perplexity 1.0032\n",
      "Epoch 99 perplexity 1.0032\n"
     ]
    }
   ],
   "source": [
    "perplexity_set = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for _ in range(epoch_size):\n",
    "            batch = next(next_batch())\n",
    "            \n",
    "            logprob_eval, _ = sess.run((logprob, optimize), {sequence: batch})\n",
    "            \n",
    "            logprob_evals.append(logprob_eval)\n",
    "            \n",
    "        saver.save(sess, os.path.join(checkpoint_dir, 'char_pred'), epoch)    \n",
    "        \n",
    "        perplexity = 2 ** -(sum(logprob_evals[-epoch_size:]) /\n",
    "                            epoch_size)\n",
    "        perplexity_set.append(perplexity)\n",
    "        print('Epoch {:2d} perplexity {:5.4f}'.format(epoch, perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "uRD3eltIVq2v",
    "outputId": "4c25db2b-83ae-464a-83ec-15a9186be3b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(perplexity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rafHL13SAMK0"
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coLjBl88AN5Q"
   },
   "outputs": [],
   "source": [
    "PRED_SEQUENCE_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4Lq0TfQAy7W"
   },
   "outputs": [],
   "source": [
    "def one_hot(batch, pred_sequence_length):\n",
    "    one_hot_batch = np.zeros((len(batch), pred_sequence_length, len(VOCABULARY)))\n",
    "\n",
    "    # Iterate through every line of text in a batch\n",
    "    for index, line in enumerate(batch):\n",
    "        line = [x for x in line if x in lookup]\n",
    "#         assert 2 <= len(line) <= MAX_SEQUENCE_LENGTH\n",
    "        \n",
    "        # Iterate through every character in a line\n",
    "        for offset, character in enumerate(line):\n",
    "            code = lookup[character]\n",
    "            one_hot_batch[index, offset, code] = 1\n",
    "    \n",
    "    return one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_5zBcCjA6ag"
   },
   "outputs": [],
   "source": [
    "def get_mask(target):\n",
    "    mask = tf.reduce_max(tf.abs(target), reduction_indices=2)\n",
    "    return mask\n",
    "\n",
    "def get_sequence_length(target):\n",
    "    mask = get_mask(target)\n",
    "    sequence_length = tf.reduce_sum(mask, reduction_indices=1)\n",
    "    \n",
    "    return sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGuaEsinA8uv"
   },
   "outputs": [],
   "source": [
    "num_neurons = 200\n",
    "cell_layers = 2\n",
    "\n",
    "num_classes = len(VOCABULARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbiIq_oPA-Kw"
   },
   "outputs": [],
   "source": [
    "def build_rnn(data, num_steps, sequence_length, initial=None):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_neurons)\n",
    "\n",
    "    multi_cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(num_neurons) for _ in range(cell_layers)])\n",
    "\n",
    "    output, state = tf.nn.dynamic_rnn(\n",
    "        inputs=data,\n",
    "        cell=multi_cell,\n",
    "        dtype=tf.float32,\n",
    "        initial_state=initial,\n",
    "        sequence_length=sequence_length)\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal([num_neurons, num_classes], stddev=0.01))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "\n",
    "    flattened_output = tf.reshape(output, [-1, num_neurons])\n",
    "\n",
    "    prediction = tf.nn.softmax(tf.matmul(flattened_output, weight) + bias)\n",
    "    prediction = tf.reshape(prediction, [-1, num_steps, num_classes])\n",
    "\n",
    "    return prediction, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-O5nGLQA_oQ"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SYcZQE7BA3w"
   },
   "outputs": [],
   "source": [
    "sequence = tf.placeholder(tf.float32, [1, PRED_SEQUENCE_LENGTH, len(VOCABULARY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOvE2c8tBCKg"
   },
   "outputs": [],
   "source": [
    "X = tf.slice(sequence, (0, 0, 0), (-1, PRED_SEQUENCE_LENGTH - 1, -1))\n",
    "y = tf.slice(sequence, (0, 1, 0), (-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0TTm_UXBICB"
   },
   "outputs": [],
   "source": [
    "state1 = tf.placeholder(tf.float32, [1, num_neurons])\n",
    "state2 = tf.placeholder(tf.float32, [1, num_neurons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "y4_PXy4pBJeA",
    "outputId": "5d84c015-23ed-4555-93b5-f66e1645c242"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder_1:0' shape=(1, 200) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_2:0' shape=(1, 200) dtype=float32>)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state1, state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yb0J2WhBLFB"
   },
   "outputs": [],
   "source": [
    "sequence_length = get_sequence_length(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LY0UGa_MBMeQ"
   },
   "outputs": [],
   "source": [
    "prediction, output = build_rnn(X, num_steps=PRED_SEQUENCE_LENGTH - 1,\n",
    "                               sequence_length=sequence_length, initial=(state1, state2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLBywfd3BN0g"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './sample_checkpoint_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzUF0kssBSIR"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6yyQiLABTSQ"
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "atUmQSffBUcA",
    "outputId": "ca9fe241-3c76-409a-a5d4-289fbeea30d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sample_checkpoint_output/char_pred-99'"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "FbAC9ErWBVyQ",
    "outputId": "24d018a1-36a7-4f7f-d030-3e05134274ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./sample_checkpoint_output/char_pred-99\n"
     ]
    }
   ],
   "source": [
    "if checkpoint and checkpoint.model_checkpoint_path:\n",
    "    tf.train.Saver().restore(sess, checkpoint.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPTQoig9BXeQ"
   },
   "outputs": [],
   "source": [
    "gen_seed = 'deep learning is'\n",
    "gen_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAIHkKtdBZVA"
   },
   "outputs": [],
   "source": [
    "curr_state1 = np.zeros((1, num_neurons))\n",
    "curr_state2 = np.zeros((1, num_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WF1xwFA7Ba-Q"
   },
   "outputs": [],
   "source": [
    "gen_text = gen_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZZbkdQQBcPR"
   },
   "outputs": [],
   "source": [
    "sampling_temperature = 0.3\n",
    "\n",
    "def sample(dist):\n",
    "    dist = np.log(dist) / sampling_temperature\n",
    "    dist = np.exp(dist) / np.exp(dist).sum()\n",
    "    choice = np.random.choice(len(dist), p=dist)\n",
    "    choice = VOCABULARY[choice]\n",
    "\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2S15UPTBdZQ"
   },
   "outputs": [],
   "source": [
    "for _ in range(gen_length):\n",
    "    feed = {\n",
    "        state1: curr_state1,\n",
    "        state2: curr_state2,\n",
    "        sequence: one_hot([gen_text[-1] + '?'], pred_sequence_length=PRED_SEQUENCE_LENGTH)\n",
    "    }\n",
    "\n",
    "    # Feed the last recurrent activation to initialize our RNN\n",
    "    gen_prediction_eval, (curr_state1, curr_state2) = sess.run(\n",
    "        [prediction, output], feed)\n",
    "    \n",
    "    # Predict just the next character\n",
    "    gen_text += sample(gen_prediction_eval[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7P7PSI55BetR",
    "outputId": "ea1c8836-ccec-4dd0-f456-aab200f7d5cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep learning is the probabilistic programming language models. There is the first success in sumble understanding the deep learning and computational describing signals, extensively supervised layers that is the first speach in the givelss of deep learning models. While therefore efficient designation, language model and a deep neural networks and the state-of-the-art performance of the classification accuracy. In this paper, we propose a novel method many an learning algorithms. The design and deep learning models. In this paper, we arsistent our sorst efficient deep neural networks are also similarity metrod. This paper descess process is struding speech recognition, language modeling, the proposed an applications such as computer vision, such as computer vision, natural language processing the proposed analysis a salge natural language processing (PLP)) and enable processing and processing and the state-of-the-art performance of a simple instance networks in computer vision, speech recognition, sp'"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9o0GJFRLW5I2",
    "outputId": "53d3fa92-3852-4f88-d727-a34c34a67d33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0178314100572678,\n",
       " 1.017829069999531,\n",
       " 1.017797500981936,\n",
       " 1.0166102859742132,\n",
       " 1.0125632828041216,\n",
       " 1.0124153685721757,\n",
       " 1.0123682102122533,\n",
       " 1.0123502106733404,\n",
       " 1.012342985868223,\n",
       " 1.01232586338298,\n",
       " 1.0123083751810387,\n",
       " 1.012299598493044,\n",
       " 1.0122921148010204,\n",
       " 1.012292093732867,\n",
       " 1.0122612690630135,\n",
       " 1.0122329706671913,\n",
       " 1.0121290914207122,\n",
       " 1.011903173374025,\n",
       " 1.011470149165765,\n",
       " 1.010932393497813,\n",
       " 1.0105647211990012,\n",
       " 1.010254330812487,\n",
       " 1.0099649412989449,\n",
       " 1.0096500743045216,\n",
       " 1.0094212895519483,\n",
       " 1.009173358606904,\n",
       " 1.0089421579196596,\n",
       " 1.008709080052112,\n",
       " 1.0085006777659142,\n",
       " 1.0083014852728818,\n",
       " 1.0080909986577513,\n",
       " 1.0079178288932447,\n",
       " 1.0077382837003535,\n",
       " 1.0075705337609204,\n",
       " 1.00741092660577,\n",
       " 1.0072633813876497,\n",
       " 1.0071034764859577,\n",
       " 1.0069841779620288,\n",
       " 1.0068396076549764,\n",
       " 1.0067295084047212,\n",
       " 1.0065967795719557,\n",
       " 1.006496109086527,\n",
       " 1.0063931664254988,\n",
       " 1.0062762905179894,\n",
       " 1.006174323094891,\n",
       " 1.0060920108461473,\n",
       " 1.005988926875571,\n",
       " 1.0059310675352768,\n",
       " 1.0058339171185706,\n",
       " 1.0057396717819516,\n",
       " 1.0056545818440592,\n",
       " 1.0055777943953959,\n",
       " 1.0055060415543038,\n",
       " 1.0054335603335856,\n",
       " 1.00538074306873,\n",
       " 1.005307675501878,\n",
       " 1.005232601826793,\n",
       " 1.0051827147593098,\n",
       " 1.0051211394530946,\n",
       " 1.0050607930865836,\n",
       " 1.004982724728728,\n",
       " 1.004943524710652,\n",
       " 1.0048823210466702,\n",
       " 1.0048171898806175,\n",
       " 1.0047483327340694,\n",
       " 1.004711135048708,\n",
       " 1.0046619822486502,\n",
       " 1.004629381710974,\n",
       " 1.0045662743420694,\n",
       " 1.0045177364432887,\n",
       " 1.0044647267612457,\n",
       " 1.0044262803411297,\n",
       " 1.0043816525678997,\n",
       " 1.0043474707421132,\n",
       " 1.0042859584652095,\n",
       " 1.0042477580855023,\n",
       " 1.0041925992042025,\n",
       " 1.0041576436637631,\n",
       " 1.0040960331775848,\n",
       " 1.0040519584839709,\n",
       " 1.0040159073424335,\n",
       " 1.0039564030665402,\n",
       " 1.003912775681661,\n",
       " 1.003865164014127,\n",
       " 1.0038454563188475,\n",
       " 1.003787111617328,\n",
       " 1.0037590734358381,\n",
       " 1.0037029507301136,\n",
       " 1.0036614880523993,\n",
       " 1.0036126146492557,\n",
       " 1.003585804769918,\n",
       " 1.0035323519272883,\n",
       " 1.003495514531153,\n",
       " 1.0034496617660504,\n",
       " 1.0033994565441082,\n",
       " 1.0033662894613449,\n",
       " 1.0033289003226538,\n",
       " 1.0032828977497121,\n",
       " 1.0032462638864146,\n",
       " 1.0031934136241794]"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Yaelg_gpP99"
   },
   "outputs": [],
   "source": [
    "input_set = list(range(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Iy8A5ZSmpcm0",
    "outputId": "27c29654-45e8-4600-954f-e275af0b3fc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "2Y2T0_tMpd3p",
    "outputId": "61fc8dc2-e8ca-4885-e2a7-4e6268447700"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOuX/A0sBQvQJEdR8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(input_set, perplexity_set)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Character_Prediction_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
